# RAGAS Evaluation Framework - DocumentaciÃ³n

## ğŸ“Š Â¿QuÃ© es RAGAS?

RAGAS (RAG Assessment) es un framework para evaluar sistemas de pregunta y respuesta basados en Retrieval-Augmented Generation. Proporciona mÃ©tricas automÃ¡ticas para medir la calidad de tu RAG.

## ğŸ¯ MÃ©tricas Disponibles

### 1. **Faithfulness** (Fidelidad)
- **Â¿QuÃ© mide?**: Â¿La respuesta estÃ¡ basada en el contexto recuperado?
- **Rango**: 0 a 1 (donde 1 es mejor)
- **InterpretaciÃ³n**:
  - 0.8+: La respuesta es fiel al contexto
  - 0.6-0.8: La mayorÃ­a estÃ¡ basada en el contexto
  - <0.6: La respuesta tiene mucha informaciÃ³n fuera del contexto

### 2. **Answer Relevancy** (Relevancia de Respuesta)
- **Â¿QuÃ© mide?**: Â¿QuÃ© tan relevante es la respuesta a la pregunta?
- **Rango**: 0 a 1
- **InterpretaciÃ³n**:
  - 0.8+: Muy relevante y directa
  - 0.6-0.8: Relevante pero con informaciÃ³n extra
  - <0.6: Poco relevante o no responde la pregunta

### 3. **Context Relevancy** (Relevancia del Contexto)
- **Â¿QuÃ© mide?**: Â¿QuÃ© tan relevante es el contexto recuperado?
- **Rango**: 0 a 1
- **InterpretaciÃ³n**:
  - 0.8+: Contexto muy relevante
  - 0.6-0.8: Contexto Ãºtil pero con ruido
  - <0.6: Contexto poco relevante

### 4. **Context Precision** (PrecisiÃ³n del Contexto)
- **Â¿QuÃ© mide?**: Â¿QuÃ© tan preciso y limpio es el contexto?
- **Rango**: 0 a 1
- **InterpretaciÃ³n**:
  - 0.8+: Contexto muy preciso
  - 0.6-0.8: Contexto con algunas partes innecesarias
  - <0.6: Mucho ruido en el contexto

## ğŸš€ CÃ³mo Usar

### OpciÃ³n 1: EvaluaciÃ³n Completa con RAGAS

```bash
# Ejecuta evaluaciÃ³n RAGAS
python ragas_evaluator.py
```

**Output esperado**:
- GenerarÃ¡ dataset de prueba con 10 preguntas
- EvaluarÃ¡ cada metrica
- GuardarÃ¡ resultados en `ragas_results.json` y `ragas_results.csv`
- MostrarÃ¡ resumen visual

**Tiempo estimado**: 5-15 minutos (depende de conexiÃ³n a API)

### OpciÃ³n 2: Monitoreo de MÃ©tricas

```bash
# Ver estado rÃ¡pido
python metrics_dashboard.py status

# Ver histÃ³rico completo
python metrics_dashboard.py dashboard

# Exportar reporte
python metrics_dashboard.py export csv
python metrics_dashboard.py export html
```

### OpciÃ³n 3: IntegraciÃ³n en tu CÃ³digo

```python
from rag_metrics_integration import (
    measure_rag_performance,
    quick_quality_check,
    print_quality_report,
    save_session_metrics
)
from rag_gemini import rag_answer

# Decorador para medir performance automÃ¡ticamente
@measure_rag_performance
def my_query(query):
    return rag_answer(query)

# Usar
result = my_query("Â¿QuÃ© son los macronutrientes?")

# EvaluaciÃ³n rÃ¡pida sin RAGAS
quality = quick_quality_check(
    query=result['query'],
    context=result['context_text'],
    answer=result['answer']
)
print_quality_report(quality)

# Guardar mÃ©tricas de sesiÃ³n
save_session_metrics("my_session_metrics.json")
```

## ğŸ“ˆ Interpretando Resultados

### Ejemplo de Reporte RAGAS

```
===============================================================
ğŸ“ˆ RAGAS EVALUATION SUMMARY
===============================================================

ğŸ“… EvaluaciÃ³n: 2024-11-24T22:30:00.123456
â“ Total de preguntas: 10

-----------  MÃ‰TRICAS PRINCIPALES -----------

ğŸŸ¢ FAITHFULNESS
  Mean:  0.8523
  Min:   0.7100
  Max:   0.9200

ğŸŸ¡ ANSWER_RELEVANCY
  Mean:  0.6845
  Min:   0.5200
  Max:   0.8900

ğŸŸ¢ CONTEXT_RELEVANCY
  Mean:  0.7892
  Min:   0.6500
  Max:   0.9100

ğŸŸ¢ CONTEXT_PRECISION
  Mean:  0.8234
  Min:   0.7000
  Max:   0.9400
```

### InterpretaciÃ³n:
- **Faithfulness 0.85**: Â¡Excelente! Las respuestas estÃ¡n basadas en contexto
- **Answer Relevancy 0.68**: Bien, pero hay mejora posible
- **Context Relevancy 0.79**: Bueno, recuperas contexto relevante
- **Context Precision 0.82**: Excelente precisiÃ³n de contexto

## ğŸ”§ ConfiguraciÃ³n

### Variables Modificables en `ragas_evaluator.py`

```python
# Preguntas de prueba
TEST_QUERIES = [
    "Tu pregunta 1",
    "Tu pregunta 2",
    # AÃ±ade mÃ¡s...
]

# ParÃ¡metros de recuperaciÃ³n
k_text = 1      # NÃºmero de documentos de texto a recuperar
k_img = 1       # NÃºmero de imÃ¡genes a recuperar
```

### Variables Modificables en `rag_metrics_integration.py`

```python
# Umbrales de calidad mÃ­nima
min_context_length = 100   # Caracteres mÃ­nimos esperados
min_response_length = 50   # Caracteres mÃ­nimos en respuesta
```

## ğŸ“Š Archivos Generados

- **ragas_results.json**: Resultados detallados de RAGAS
- **ragas_results.csv**: Mismos resultados en formato tabular
- **ragas_evaluations/latest_results.json**: Ãšltimo resultado
- **ragas_evaluations/metrics_history.csv**: HistÃ³rico de evaluaciones
- **session_metrics.json**: MÃ©tricas de sesiÃ³n actual
- **metrics_report_YYYYMMDD_HHMMSS.csv**: Reportes exportados

## ğŸ¯ Mejora de Resultados

### Si FAITHFULNESS es baja (<0.6):
- âœ“ AÃ±ade mÃ¡s documentos de referencia
- âœ“ Mejora prompts para incluir "Solo usa contexto"
- âœ“ Verifica que el retriever devuelva documentos relevantes

### Si ANSWER_RELEVANCY es baja (<0.6):
- âœ“ Refina prompts para ser mÃ¡s directos
- âœ“ Aumenta nÃºmero de documentos recuperados
- âœ“ Mejora embedding model

### Si CONTEXT_RELEVANCY es baja (<0.6):
- âœ“ Mejora embedding model
- âœ“ Aumenta nÃºmero de resultados recuperados
- âœ“ Ajusta threshold de similitud

### Si CONTEXT_PRECISION es baja (<0.6):
- âœ“ Reduce nÃºmero de documentos recuperados
- âœ“ Mejora calidad de chunking
- âœ“ Filtra documentos menos relevantes

## ğŸ’¡ Mejores PrÃ¡cticas

1. **Ejecuta regularmente**: Al menos una vez por semana
2. **MantÃ©n histÃ³rico**: Monitorea tendencias de calidad
3. **Usa mÃºltiples queries**: Variedad en tus preguntas de prueba
4. **Combina mÃ©tricas**: No confÃ­es en una sola mÃ©trica
5. **Valida manualmente**: Revisa respuestas que tengas dudas
6. **Documenta cambios**: Registra quÃ© cambios afectaron mÃ©tricas

## ğŸš¨ Troubleshooting

### Error: "GOOGLE_API_KEY no encontrada"
```bash
# Crea .env si no existe
echo "GOOGLE_API_KEY=tu_api_key" > .env
```

### Error: "No module named ragas"
```bash
pip install ragas
```

### Error: EvaluaciÃ³n muy lenta
- Reduce TEST_QUERIES a 5 preguntas
- Usa modelo mÃ¡s ligero en `gemini_llm.py`
- Aumenta `k_text` y reduce `k_img`

### Resultados inconsistentes entre ejecuciones
- Normal para LLMs, la aleatoriedad es inherente
- Ejecuta varias veces y promedÃ­a resultados
- Usa mÃ©tricas determinÃ­sticas como context_precision

## ğŸ“š Recursos

- [RAGAS GitHub](https://github.com/explodinggradients/ragas)
- [RAGAS Docs](https://docs.ragas.io/)
- [Paper RAGAS](https://arxiv.org/abs/2309.15217)

## ğŸ”„ Flujo Recomendado

```
1. Ejecutar ragas_evaluator.py
   â†“
2. Revisar ragas_results.json
   â†“
3. Si scores < 0.7 â†’ Hacer mejoras en cÃ³digo
   â†“
4. Ejecutar metrics_dashboard.py dashboard
   â†“
5. Comparar histÃ³rico con ejecuciÃ³n anterior
   â†“
6. Documentar cambios y resultados
   â†“
7. Repetir semanalmente
```

## âœ… Checklist de Calidad

- [ ] Faithfulness > 0.75
- [ ] Answer Relevancy > 0.70
- [ ] Context Relevancy > 0.75
- [ ] Context Precision > 0.75
- [ ] Latencia promedio < 5 segundos
- [ ] Tasa de errores < 5%
- [ ] HistÃ³rico muestra tendencia positiva

Â¡Si cumples estos criterios, tu RAG estÃ¡ en buen estado! ğŸ‰
