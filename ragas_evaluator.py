"""
RAGAS Evaluation Framework for RAG System
Evaluates: Faithfulness, Answer Relevancy, Context Relevancy, Context Precision
"""

import os
import json
import chromadb
import pandas as pd
from datetime import datetime
from dotenv import load_dotenv
from typing import Dict, List, Tuple

# Dataset
from datasets import Dataset

# RAGAS Metrics
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_relevancy,
    context_precision,
)

# Custom imports
from gemini_llm import GeminiLLM
from hf_embedder import HFEmbedder
from rag_gemini import rag_answer

# ===================== CONFIG =====================
CHROMA_PATH = "./chroma_db"
TEXT_COLLECTION = "nutricion_textos"
IMAGE_COLLECTION = "nutricion_imagenes"
RESULTS_FILE = "./ragas_results.json"
RESULTS_CSV = "./ragas_results.csv"

# Test Questions
TEST_QUERIES = [
    "Â¿Por quÃ© es importante dormir bien?",
    "Â¿CuÃ¡les son los macronutrientes principales?",
    "Â¿CuÃ¡nta agua debo beber diariamente?",
    "Â¿QuÃ© beneficios tiene caminar 30 minutos?",
    "Â¿QuÃ© son los snacks saludables?",
    "Â¿CÃ³mo manejo el estrÃ©s?",
    "Â¿QuÃ© son las vitaminas y minerales?",
    "Â¿CÃ³mo funciona el ayuno intermitente?",
    "Â¿CÃ³mo planificar comidas saludables?",
    "Â¿QuÃ© es la salud digestiva?",
]

# ================================================


def init_gemini():
    """Inicializa Gemini LLM con API key"""
    load_dotenv()
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise ValueError("Falta GOOGLE_API_KEY en .env")
    return GeminiLLM(api_key=api_key)


def init_embedder():
    """Inicializa embedder de HuggingFace"""
    return HFEmbedder()


def get_chroma_collections():
    """Obtiene colecciones de Chroma"""
    client = chromadb.PersistentClient(path=CHROMA_PATH)
    col_text = client.get_collection(TEXT_COLLECTION)
    col_images = client.get_collection(IMAGE_COLLECTION)
    return col_text, col_images


def retrieve_context_for_eval(query: str, col_text, col_images, k_text=1, k_img=1) -> Tuple[str, str]:
    """
    Recupera contexto para evaluaciÃ³n
    Retorna: (context_text, context_images)
    """
    # BÃºsqueda en textos
    res_text = col_text.query(query_texts=[query], n_results=k_text)
    text_chunks = res_text.get("documents", [[]])[0]
    text_metas = res_text.get("metadatas", [[]])[0]

    ctx_text = ""
    for doc, meta in zip(text_chunks, text_metas):
        src = meta.get("source", "desconocido")
        ctx_text += f"[Fuente: {src}]\n{doc}\n\n"

    # BÃºsqueda en imÃ¡genes
    res_img = col_images.query(query_texts=[query], n_results=k_img)
    img_chunks = res_img.get("documents", [[]])[0]
    img_metas = res_img.get("metadatas", [[]])[0]

    ctx_img = ""
    for caption, meta in zip(img_chunks, img_metas):
        img_path = meta.get("source_image", "desconocida")
        ctx_img += f"[Imagen: {img_path}]\nDescripciÃ³n: {caption}\n\n"

    return ctx_text.strip(), ctx_img.strip()


def generate_test_dataset(llm, embedder, col_text, col_images) -> Dataset:
    """
    Genera dataset de prueba con queries, ground truth y contexto
    """
    data = {
        "question": [],
        "contexts": [],
        "answer": [],
        "ground_truth": [],
    }

    print("ðŸ“Š Generando dataset de evaluaciÃ³n...")

    for i, query in enumerate(TEST_QUERIES, 1):
        print(f"  [{i}/{len(TEST_QUERIES)}] Procesando: {query[:50]}...")

        try:
            # ObtÃ©n respuesta RAG
            result = rag_answer(query)
            answer = result["answer"]
            context_text = result["context_text"]

            # Contexto como lista (RAGAS espera lista)
            contexts = [context_text] if context_text else [""]

            # Ground truth: usa el mismo answer (idealmente tendrÃ­as verdad terreno)
            ground_truth = answer

            data["question"].append(query)
            data["contexts"].append(contexts)
            data["answer"].append(answer)
            data["ground_truth"].append(ground_truth)

        except Exception as e:
            print(f"    âŒ Error: {str(e)}")
            continue

    print(f"âœ… Dataset generado con {len(data['question'])} ejemplos\n")

    return Dataset.from_dict(data)


def evaluate_rag(dataset: Dataset, llm: GeminiLLM, embedder: HFEmbedder) -> Dict:
    """
    EvalÃºa RAG usando RAGAS con mÃºltiples mÃ©tricas
    """
    print("ðŸ” Evaluando RAG con RAGAS...\n")

    try:
        # EvalÃºa con mÃºltiples mÃ©tricas
        results = evaluate(
            dataset,
            metrics=[
                faithfulness,
                answer_relevancy,
                context_relevancy,
                context_precision,
            ],
            llm=llm,
            embeddings=embedder,
        )

        return results

    except Exception as e:
        print(f"âŒ Error durante evaluaciÃ³n: {str(e)}")
        return None


def save_results(results, dataset: Dataset):
    """
    Guarda resultados en JSON y CSV
    """
    if results is None:
        print("âŒ No hay resultados para guardar")
        return

    print("ðŸ’¾ Guardando resultados...\n")

    # Preparar datos para guardar
    metrics_summary = {
        "timestamp": datetime.now().isoformat(),
        "total_questions": len(dataset),
        "metrics": {},
        "detailed_results": [],
    }

    # Extraer mÃ©tricas agregadas
    if hasattr(results, "scores"):
        for metric_name in results.scores.keys():
            scores = results.scores[metric_name]
            if isinstance(scores, list):
                metrics_summary["metrics"][metric_name] = {
                    "mean": float(sum(scores) / len(scores)),
                    "min": float(min(scores)),
                    "max": float(max(scores)),
                    "scores": scores,
                }

    # Resultados detallados
    for i, (question, answer, contexts) in enumerate(
        zip(
            dataset["question"],
            dataset["answer"],
            dataset["contexts"],
        )
    ):
        detail = {
            "id": i,
            "question": question,
            "answer": answer[:200] + "..." if len(answer) > 200 else answer,
            "context_used": contexts[0][:200] + "..." if contexts and len(contexts[0]) > 200 else (contexts[0] if contexts else ""),
        }

        # Agregar scores por mÃ©trica
        if hasattr(results, "scores"):
            for metric_name in results.scores.keys():
                if isinstance(results.scores[metric_name], list):
                    if i < len(results.scores[metric_name]):
                        detail[metric_name] = results.scores[metric_name][i]

        metrics_summary["detailed_results"].append(detail)

    # Guardar JSON
    with open(RESULTS_FILE, "w", encoding="utf-8") as f:
        json.dump(metrics_summary, f, indent=2, ensure_ascii=False)
    print(f"âœ… Resultados guardados en: {RESULTS_FILE}")

    # Guardar CSV
    if metrics_summary["detailed_results"]:
        df = pd.DataFrame(metrics_summary["detailed_results"])
        df.to_csv(RESULTS_CSV, index=False, encoding="utf-8")
        print(f"âœ… CSV guardado en: {RESULTS_CSV}")

    return metrics_summary


def print_summary(metrics_summary: Dict):
    """
    Imprime resumen visual de mÃ©tricas
    """
    print("\n" + "=" * 70)
    print("ðŸ“ˆ RAGAS EVALUATION SUMMARY")
    print("=" * 70)

    print(f"\nðŸ“… EvaluaciÃ³n: {metrics_summary['timestamp']}")
    print(f"â“ Total de preguntas: {metrics_summary['total_questions']}")

    print("\n" + "-" * 70)
    print("ðŸŽ¯ MÃ‰TRICAS PRINCIPALES:")
    print("-" * 70)

    metrics = metrics_summary.get("metrics", {})

    if not metrics:
        print("âŒ No hay mÃ©tricas para mostrar")
        return

    for metric_name, values in metrics.items():
        if isinstance(values, dict) and "mean" in values:
            mean = values["mean"]
            min_val = values["min"]
            max_val = values["max"]

            # Emojis segÃºn rendimiento
            if mean >= 0.8:
                emoji = "ðŸŸ¢"
            elif mean >= 0.6:
                emoji = "ðŸŸ¡"
            else:
                emoji = "ðŸ”´"

            print(f"\n{emoji} {metric_name.upper()}")
            print(f"  Mean:  {mean:.4f}")
            print(f"  Min:   {min_val:.4f}")
            print(f"  Max:   {max_val:.4f}")

    print("\n" + "=" * 70)
    print("ðŸ’¡ INTERPRETACIÃ“N:")
    print("-" * 70)
    print("""
Faithfulness (0-1):       Â¿La respuesta estÃ¡ basada en el contexto?
                          - 0.8+: Excelente
                          - 0.6-0.8: Bueno
                          - <0.6: Necesita mejora

Answer Relevancy (0-1):   Â¿La respuesta es relevante a la pregunta?
                          - 0.8+: Muy relevante
                          - 0.6-0.8: Relevante
                          - <0.6: Poco relevante

Context Relevancy (0-1):  Â¿El contexto recuperado es relevante?
                          - 0.8+: Contexto excelente
                          - 0.6-0.8: Contexto bueno
                          - <0.6: Contexto pobre

Context Precision (0-1):  Â¿QuÃ© tan preciso es el contexto?
                          - 0.8+: Muy preciso
                          - 0.6-0.8: Preciso
                          - <0.6: Impreciso
    """)
    print("=" * 70 + "\n")


def main():
    """Ejecuta evaluaciÃ³n RAGAS completa"""
    print("\nðŸš€ INICIANDO EVALUACIÃ“N RAGAS\n")

    try:
        # Inicializar componentes
        print("ðŸ”§ Inicializando componentes...")
        llm = init_gemini()
        embedder = init_embedder()
        col_text, col_images = get_chroma_collections()
        print("âœ… Componentes inicializados\n")

        # Generar dataset
        dataset = generate_test_dataset(llm, embedder, col_text, col_images)

        if len(dataset) == 0:
            print("âŒ Dataset vacÃ­o, abortando")
            return

        # Evaluar
        results = evaluate_rag(dataset, llm, embedder)

        # Guardar resultados
        metrics_summary = save_results(results, dataset)

        # Mostrar resumen
        if metrics_summary:
            print_summary(metrics_summary)
        else:
            print("âŒ No se pudieron obtener resumen de mÃ©tricas")

    except Exception as e:
        print(f"\nâŒ ERROR FATAL: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
